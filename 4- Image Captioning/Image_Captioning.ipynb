{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image_Captioning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qm3t5oQrzUa5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from utils import save_checkpoint, load_checkpoint, print_examples\n",
        "from get_loader import get_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderCNN(nn.Module):\n",
        "  def __init__(self, embed_size, train_CNN = False):\n",
        "    super(EncoderCNN, self).__init__()\n",
        "    self.train_CNN = train_CNN\n",
        "    # We do not train a new model, instead funetuning the last layer\n",
        "    self.inception = models.inception_v3(pretrained= True, aux_logits = False)\n",
        "    self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n",
        "    # Removing the last layer of inception_v3 and send the in_features to\n",
        "    # a new linear layer.\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "\n",
        "  def forward(self, images):\n",
        "    features = self.inception(images)\n",
        "\n",
        "    for name, param in self.inception.named_parameters():\n",
        "      if \"fc.weight\" in name or 'fc.bias' in name:\n",
        "        param.requires_grad = True\n",
        "      \n",
        "      else:\n",
        "        param.requires_grad = self.train_CNN\n",
        "\n",
        "    return self.dropout(self.relu(features))"
      ],
      "metadata": {
        "id": "LGC4wEbxz2U8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "  def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
        "    super(DecoderRNN, self).__init__()\n",
        "    self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "    self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n",
        "    self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "\n",
        "  def forward(self, features, captions):\n",
        "    embeddings = self.dropout(self.embed(captions))\n",
        "    embeddings = torch.cat((features.unsqueeze(0), embeddings), dim = 0)\n",
        "    hiddens, _ = self.lstm(embeddings)\n",
        "    output = self.linear(hiddens)\n",
        "\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "clJK0pvW2IIb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNtoRNN(nn.Module):\n",
        "  def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
        "    super(CNNtoRNN, self).__init__()\n",
        "    self.encoderCNN = EncoderCNN(embed_size)\n",
        "    self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
        "\n",
        "\n",
        "  def forward(self, images, captions):\n",
        "    features = self.encoderCNN(images)\n",
        "    outputs = self.decoderRNN(features, captions)\n",
        "    return outputs\n",
        "\n",
        "  \n",
        "  def caption_image(self, image, vocabulary, max_length = 50):\n",
        "    result_caption = []\n",
        "    with torch.no_grad():\n",
        "      x = self.encoderCNN(image).unsqueeze(0)\n",
        "      states = None\n",
        "\n",
        "      for _ in range(max_length):\n",
        "        hiddens, states = self.decoderRNN.lstm(x, states)\n",
        "        output = self.decoderRNN.linear(hiddens.squeeze(0))\n",
        "        predicted = output.argmax(1)\n",
        "\n",
        "        result_caption.append(predicted.item())\n",
        "\n",
        "        x = self.decoderRNN.embed(predicted).unsqueeze(0)\n",
        "\n",
        "        if vocabulary.itos[predicted.item()] == '<EOS>':\n",
        "          break\n",
        "      return [vocabulary.itos[idx] for idx in result_caption]\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NnxqpnKj223q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize((356, 356)),\n",
        "            transforms.RandomCrop((299, 299)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    train_loader, dataset = get_loader(\n",
        "        root_folder=\"/content/flickr8k/images\",\n",
        "        annotation_file=\"/content/flickr8k/captions.txt\",\n",
        "        transform=transform,\n",
        "        num_workers=2,\n",
        "    )\n",
        "\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    load_model = False\n",
        "    save_model = False\n",
        "    train_CNN = False\n",
        "\n",
        "    # Hyperparameters\n",
        "    embed_size = 256\n",
        "    hidden_size = 256\n",
        "    vocab_size = len(dataset.vocab)\n",
        "    num_layers = 1\n",
        "    learning_rate = 3e-4\n",
        "    num_epochs = 10\n",
        "\n",
        "    # for tensorboard\n",
        "    writer = SummaryWriter(\"runs/flickr\")\n",
        "    step = 0\n",
        "\n",
        "    # initialize model, loss etc\n",
        "    model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Only finetune the CNN\n",
        "    for name, param in model.encoderCNN.inception.named_parameters():\n",
        "        if \"fc.weight\" in name or \"fc.bias\" in name:\n",
        "            param.requires_grad = True\n",
        "        else:\n",
        "            param.requires_grad = train_CNN\n",
        "\n",
        "    if load_model:\n",
        "        step = load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Uncomment the line below to see a couple of test cases\n",
        "        # print_examples(model, device, dataset)\n",
        "\n",
        "        if save_model:\n",
        "            checkpoint = {\n",
        "                \"state_dict\": model.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "                \"step\": step,\n",
        "            }\n",
        "            save_checkpoint(checkpoint)\n",
        "\n",
        "        for idx, (imgs, captions) in tqdm(\n",
        "            enumerate(train_loader), total=len(train_loader), leave=False\n",
        "        ):\n",
        "            imgs = imgs.to(device)\n",
        "            captions = captions.to(device)\n",
        "\n",
        "            outputs = model(imgs, captions[:-1])\n",
        "            loss = criterion(\n",
        "                outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1)\n",
        "            )\n",
        "\n",
        "            writer.add_scalar(\"Training loss\", loss.item(), global_step=step)\n",
        "            step += 1\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward(loss)\n",
        "            optimizer.step()"
      ],
      "metadata": {
        "id": "GWavdFPg97qW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "id": "8w73L5aa98l8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}